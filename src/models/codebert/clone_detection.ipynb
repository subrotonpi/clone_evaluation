{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, argparse, re, json, random, copy, logging\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import argmax, save, load, sum, sqrt\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from transformers import (RobertaConfig, RobertaModel, RobertaTokenizer)\n",
    "from transformers import WEIGHTS_NAME, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from transformers import WEIGHTS_NAME, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW \n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "# import helpers_data_process as dproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size*2, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = x.reshape(-1,x.size(-1)*2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "        \n",
    "class Model(nn.Module):   \n",
    "    def __init__(self, encoder,config,tokenizer,args):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.config=config\n",
    "        self.tokenizer=tokenizer\n",
    "        self.classifier=RobertaClassificationHead(config)\n",
    "        self.args=args\n",
    "    \n",
    "    def forward(self, input_ids=None,labels=None): \n",
    "        input_ids=input_ids.view(-1,self.args.block_size)\n",
    "        outputs = self.encoder(input_ids= input_ids,attention_mask=input_ids.ne(1))[0]\n",
    "        #print('here', outputs)\n",
    "        logits=self.classifier(outputs)\n",
    "        prob=F.softmax(logits, dim=-1)\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss,prob\n",
    "        else:\n",
    "            return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self, input_tokens, input_ids, id1,id2, label):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.label = label\n",
    "        self.id1 = id1\n",
    "        self.id2 = id2\n",
    "        \n",
    "\n",
    "def convert_examples_to_features(args, x): \n",
    "    \n",
    "    code_1 = x[0]\n",
    "    code_2 = x[1]\n",
    "    label = x[2]\n",
    "    tokenizer=x[3]\n",
    "    \n",
    "    id1 = x[4]\n",
    "    id2=x[5]\n",
    "\n",
    "    code1_tokens=tokenizer.tokenize(code_1)    \n",
    "    code1_tokens=code1_tokens[:args.block_size-2]\n",
    "    code1_tokens =[tokenizer.cls_token]+code1_tokens+[tokenizer.sep_token]\n",
    "    code1_ids=tokenizer.convert_tokens_to_ids(code1_tokens)\n",
    "    padding_length = args.block_size - len(code1_ids)\n",
    "    code1_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "    code2_tokens=tokenizer.tokenize(code_2)\n",
    "    code2_tokens=code2_tokens[:args.block_size-2]\n",
    "    code2_tokens =[tokenizer.cls_token]+code2_tokens+[tokenizer.sep_token]      \n",
    "    code2_ids=tokenizer.convert_tokens_to_ids(code2_tokens)\n",
    "    padding_length = args.block_size - len(code2_ids)\n",
    "    code2_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    source_tokens=code1_tokens+code2_tokens\n",
    "    source_ids=code1_ids+code2_ids\n",
    "    #input_tokens, input_ids, label, uid_func, uid_file)\n",
    "    return InputFeatures(source_tokens, source_ids, id1, id2, int(label)) #input_tokens, input_ids, id1,id2, label\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, args, tokenizer, file_path=None):\n",
    "       \n",
    "        self.examples = []\n",
    "        codes={}\n",
    "        with open(args.codes) as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                js=json.loads(line)\n",
    "                codes[js['idx']]=js['func']\n",
    "\n",
    "        # Read the CSV file with specified column names\n",
    "        f = pd.read_csv(file_path, names=[\"id1\", \"id2\", \"label\"])\n",
    "        data = []\n",
    "        for ind, row in f.iterrows():\n",
    "            id1 = row['id1']\n",
    "            id2 = row['id2']\n",
    "            label = row['label']\n",
    "            data.append((codes[id1], codes[id2], label, tokenizer, id1, id2))\n",
    "        print(len(data))\n",
    "\n",
    "        if 'valid' in file_path:\n",
    "            data = random.sample(data,int(len(data)*0.1))\n",
    "        #convert example to input features   ddd\n",
    "        for x in data:\n",
    "            try:\n",
    "                self.examples.append(convert_examples_to_features(args, x))\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):       \n",
    "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label), torch.tensor(self.examples[i].id1),torch.tensor(self.examples[i].id2)\n",
    "        \n",
    "def create_dataset(args, tokenizer, fp):\n",
    "    dx_dataset = TextDataset(args, tokenizer, fp)\n",
    "    dx_sampler = RandomSampler(dx_dataset) \n",
    "    dx_dataloader = DataLoader(dx_dataset, sampler=dx_sampler, batch_size=args.batch_size)\n",
    "    return dx_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(y_trues, y_preds):\n",
    "    accuracy= accuracy_score(y_trues, y_preds)\n",
    "    f1 = f1_score(y_trues, y_preds)\n",
    "    precision = precision_score(y_trues, y_preds)\n",
    "    recall = recall_score(y_trues, y_preds)\n",
    "    result = { \n",
    "            \"precision\": round(precision,4),\n",
    "            \"recall\": round(recall,4),\n",
    "            \"f1\": round(f1,4),\n",
    "            \"accuracy\": round(accuracy,4)\n",
    "            }\n",
    "    return result\n",
    "    \n",
    "def model_evaluation(args, model, dx_dataloader):\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for batch in dx_dataloader:\n",
    "        loss, logits_= model(batch[0].to(args.device), batch[1].to(args.device))\n",
    "        y_preds.append(logits_.detach().cpu().numpy())\n",
    "        y_trues.append(batch[1].detach().cpu().numpy())\n",
    "    # best_threshold=0.5\n",
    "    y_preds=np.concatenate(y_preds,0)\n",
    "    y_trues=np.concatenate(y_trues,0)\n",
    "    y_preds= y_preds.argmax(-1)  #y_preds[:,1]>best_threshold\n",
    "    return evaluation_metrics(y_trues, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(args, model,tokenizer):\n",
    "    train_dataloader = create_dataset(args, tokenizer, args.train_data_file)\n",
    "    max_steps=args.epochs*len( train_dataloader)\n",
    "    save_steps=len(train_dataloader)//10\n",
    "    warmup_steps=max_steps//5\n",
    "    if save_steps==0:\n",
    "        save_steps = 2\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm=1\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW (optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps)\n",
    "    global_step=0\n",
    "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
    "    best_f1=-1\n",
    "\n",
    "    #training loop\n",
    "    model.zero_grad()\n",
    "    for idx in range(args.epochs): \n",
    "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "        tr_num=0\n",
    "        train_loss=0\n",
    "        for step, batch in enumerate(bar):\n",
    "            inputs = batch[0].to(args.device)\n",
    "            labels = batch[1].to(args.device)\n",
    "        \n",
    "            model.train()\n",
    "            loss,logits = model(inputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            tr_loss += loss.item()\n",
    "            tr_num+=1\n",
    "            train_loss+=loss.item()\n",
    "            if avg_loss==0:\n",
    "                avg_loss=tr_loss               \n",
    "            avg_loss=round(train_loss/tr_num,5)\n",
    "            bar.set_description(\"epoch {} loss {}\".format(idx,avg_loss))\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()  \n",
    "                global_step += 1\n",
    "                output_flag=True\n",
    "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
    "\n",
    "                if global_step % save_steps == 0: \n",
    "                    # Save model checkpoint\n",
    "                    valid_data = create_dataset(args, tokenizer, args.valid_data_file)\n",
    "                    results = model_evaluation(args, model, valid_data)\n",
    "                    eval_f1=results['f1']\n",
    "                    if eval_f1>best_f1:\n",
    "                        \n",
    "                        best_f1=eval_f1 #results['eval_f1']                             \n",
    "                        model_to_save = model.module if hasattr(model,'module') else model\n",
    "                        output_dir = f\"{args.output_dir}/codebert_best.bin\"\n",
    "                        print(\"saving model checkpoint to \"+ output_dir)\n",
    "                        torch.save(model_to_save.state_dict(), output_dir)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    config = RobertaConfig.from_pretrained(args.pretrained_model_name)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args.pretrained_model_name)\n",
    "    model = RobertaModel.from_pretrained(args.pretrained_model_name,config=config)   \n",
    "    model = Model(model,config,tokenizer, args)\n",
    "    model.to(args.device)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    print(args)\n",
    "    set_seed(args.seed)\n",
    "    create_directory(args.output_dir)\n",
    "    #results_dict\n",
    "    results_dict = {}\n",
    "    # get model tokenizer\n",
    "    model, tokenizer = get_model(args)\n",
    "    \n",
    "    test_dataset= create_dataset(args, tokenizer, args.test_data_file)\n",
    "    \n",
    "    #BASELINE MODEL EVAL\n",
    "    print(\"evaluate on test data on baseline model\\n\")\n",
    "    results_dict['baseline'] = model_evaluation(args, model, test_dataset) \n",
    "    print(\"baseline: \", results_dict['baseline'])\n",
    "    #TRAIN AND EVAL\n",
    "    print(\"fine-tuning model\")\n",
    "    train_loop(args, model,tokenizer)\n",
    "    #eval\n",
    "    print(\"evaluate on test data using trained model\\n\")\n",
    "    results_dict['fine_tuned'] = model_evaluation(args, model, test_dataset) \n",
    "    print(\"trained model: \",results_dict['fine_tuned'])\n",
    "    \n",
    "    #BEST MODEL EVAL\n",
    "    model, tokenizer = get_model(args) #get new model to load saved model\n",
    "    model_path = f\"{args.output_dir}/codebert_best.bin\"\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc:storage), strict=False)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    print(\"evaluate on test data using best model\\n\")\n",
    "    results_dict['best_model'] = model_evaluation(args, model, test_dataset) \n",
    "    print(\"best model: \", results_dict['best_model'])\n",
    "    \n",
    "    # #BEST MODEL ON GROUND TRUTH DATA\n",
    "    # # gt_data = create_dataset(args, tokenizer, args.gt_data_file)\n",
    "    # # results_dict['best_model_gt'] = model_evaluation_and_save_preds(args, model, gt_data, \"best_model_gt\") \n",
    "    # # print(\"best_model_gt model: \", results_dict['best_model_gt'])\n",
    "    return results_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(device='cuda', epochs=1, block_size=512, batch_size=2, pretrained_model_name='microsoft/codebert-base', data='/student/egk204/projects/clone-type-iv/data', seed=42, do_train=False, test_saved_models=False, benchmark='gptcb', system_storage='./', output_dir='saved_models/checkpoint-best-f1/gptcb', dataset_path='/student/egk204/projects/clone-type-iv/data/gptcb', codes='/student/egk204/projects/clone-type-iv/data/gptcb/data.jsonl', train_data_file='/student/egk204/projects/clone-type-iv/data/gptcb/train.csv', valid_data_file='/student/egk204/projects/clone-type-iv/data/gptcb/valid.csv', test_data_file='/student/egk204/projects/clone-type-iv/data/gptcb/test.csv')\n",
      "1115\n",
      "evaluate on test data on baseline model\n",
      "\n",
      "baseline:  {'precision': 0.5495, 'recall': 0.1091, 'f1': 0.1821, 'accuracy': 0.5085}\n",
      "fine-tuning model\n",
      "8912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d26754b8d94dfaa5b6f9ab7f89f426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114\n",
      "saving model checkpoint to saved_models/checkpoint-best-f1/gptcb/codebert_best.bin\n",
      "1114\n",
      "saving model checkpoint to saved_models/checkpoint-best-f1/gptcb/codebert_best.bin\n",
      "1114\n",
      "saving model checkpoint to saved_models/checkpoint-best-f1/gptcb/codebert_best.bin\n",
      "1114\n",
      "1114\n",
      "saving model checkpoint to saved_models/checkpoint-best-f1/gptcb/codebert_best.bin\n",
      "1114\n",
      "1114\n",
      "saving model checkpoint to saved_models/checkpoint-best-f1/gptcb/codebert_best.bin\n",
      "1114\n",
      "1114\n",
      "1114\n",
      "evaluate on test data using trained model\n",
      "\n",
      "trained model:  {'precision': 0.9946, 'recall': 0.9857, 'f1': 0.9901, 'accuracy': 0.9901}\n",
      "evaluate on test data using best model\n",
      "\n",
      "best model:  {'precision': 0.9893, 'recall': 0.9893, 'f1': 0.9893, 'accuracy': 0.9892}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_486986/2960540275.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#baseline, train, saved results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mall_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# print(all_results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mall_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{args.system_storage}/codebert_results_{args.benchmark}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/clone-type-iv/venv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0morient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m                 \u001b[0;31m# TODO speed up Series case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1800\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_from_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m                     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    parser.add_argument('--device',default= \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser.add_argument('--epochs', default=1)\n",
    "    parser.add_argument('--block_size', default=512)\n",
    "    parser.add_argument('--batch_size', default=2) #16 for A6000\n",
    "    parser.add_argument('--pretrained_model_name', default='microsoft/codebert-base')\n",
    "    parser.add_argument('--data', default='/student/egk204/projects/clone-type-iv/data') \n",
    "    parser.add_argument('--seed', default= 42)\n",
    "    parser.add_argument('--do_train', default=False)\n",
    "    parser.add_argument('--test_saved_models', default=False)\n",
    "\n",
    "    parser.add_argument('--benchmark', default=\"gptcb\") #benchmark\n",
    "    # parser.add_argument('--train_data_file', default=\"\")\n",
    "    # parser.add_argument('--valid_data_file', default=\"\")\n",
    "    # parser.add_argument('--test_data_file', default=\"\")\n",
    "    args=parser.parse_args([])\n",
    "    # create_directory(\"saved_models/checkpoint-best-f1\")\n",
    "    \n",
    "    #modify required arguements: system specific\n",
    "    #\n",
    "    args.system_storage = f\"./\"\n",
    "    args.output_dir =  f\"saved_models/checkpoint-best-f1/{args.benchmark}\"\n",
    "    args.dataset_path = f\"{args.data}/{args.benchmark}\"\n",
    "    \n",
    "    #for this task these files are in specific directory, no need to pass from terminal\n",
    "    args.codes = f\"{args.dataset_path}/data.jsonl\"\n",
    "    args.train_data_file = f\"{args.dataset_path}/train.csv\"\n",
    "    args.valid_data_file = f\"{args.dataset_path}/valid.csv\"\n",
    "    args.test_data_file = f\"{args.dataset_path}/test.csv\"\n",
    "    \n",
    "    # # get model tokenizer\n",
    "    # model, tokenizer = get_model(args)\n",
    "    \n",
    "    # test_dataset= create_dataset(args, tokenizer, args.test_data_file)\n",
    "    \n",
    "    \n",
    "    #baseline, train, saved results\n",
    "    all_results = run(args)\n",
    "    # print(all_results)\n",
    "    all_results = pd.DataFrame.from_dict(all_results, orient=\"index\")\n",
    "    all_results.to_csv(f\"{args.system_storage}/codebert_results_{args.benchmark}.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
